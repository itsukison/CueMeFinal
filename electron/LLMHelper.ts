import { GoogleGenerativeAI, GenerativeModel } from "@google/generative-ai"
import fs from "fs"
import { QnAService, SearchResult } from "./QnAService"
import { DocumentService, DocumentSearchResult } from "./DocumentService"
import { ModeManager } from "./ModeManager"
import { ModeResponse, CompatibleResponse } from "../src/types/modes"

export interface RAGContext {
  hasContext: boolean
  results: SearchResult[]
  documentChunks?: DocumentSearchResult[]
  collectionName?: string
  documentName?: string
  type: 'qna' | 'document'
}

export class LLMHelper {
  private model: GenerativeModel
  private qnaService: QnAService | null = null
  private documentService: DocumentService | null = null
  private readonly modelName: string
  private readonly systemPrompt = `é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹æ—¥æœ¬èªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

ç¦æ­¢äº‹é …:
â€¢ å‰ç½®ãï¼ˆã€Œã¯ã„ã€ã€ã€Œãã‚Œã§ã¯ã€ã€ã€ŒãŠç­”ãˆã—ã¾ã™ã€ãªã©ï¼‰
â€¢ ãƒ¡ã‚¿èª¬æ˜ï¼ˆã€Œé¢æ¥å®˜ã«ä¼ãˆã‚‹å½¢ã§ã€ã€Œèª¬æ˜ã—ã¾ã™ã€ãªã©ï¼‰
â€¢ æƒ…å ±æºã¸ã®è¨€åŠï¼ˆã€Œå‚è€ƒæƒ…å ±ã«ã‚ˆã‚‹ã¨ã€ãªã©ï¼‰
â€¢ ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜æ³•ï¼ˆ**å¤ªå­—**ã€*æ–œä½“*ã€*ç®‡æ¡æ›¸ãï¼‰ã¯ä½¿ç”¨ç¦æ­¢

å›ç­”å½¢å¼:
â€¢ æ ¸å¿ƒã‚’æœ€åˆã«è¿°ã¹ã‚‹
â€¢ å…·ä½“ä¾‹ã‚’2-3å€‹å«ã‚ã‚‹
â€¢ ç®‡æ¡æ›¸ãã¯ã€Œâ€¢ã€ã‚’ä½¿ç”¨
â€¢ é‡è¦ãªèªå¥ã¯ã€ã€‘ã§å›²ã‚€ï¼ˆä¾‹: ã€é‡è¦ã€‘ï¼‰`

  constructor(apiKey: string, modelName: string = "gemini-2.0-flash") {
    const genAI = new GoogleGenerativeAI(apiKey)
    this.modelName = modelName
    this.model = genAI.getGenerativeModel({
      model: modelName,
      generationConfig: {
        temperature: 0.7,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 1200, // Balanced length for detailed but concise responses
      }
    })
    console.log(`[LLMHelper] Initialized with model: ${modelName}`);
  }

  private async fileToGenerativePart(imagePath: string) {
    const imageData = await fs.promises.readFile(imagePath)
    return {
      inlineData: {
        data: imageData.toString("base64"),
        mimeType: "image/png"
      }
    }
  }

  private cleanJsonResponse(text: string): string {
    // Remove markdown code block syntax if present
    text = text.replace(/^```(?:json)?\n/, '').replace(/\n```$/, '');
    // Remove any leading/trailing whitespace
    text = text.trim();
    return text;
  }

  public async extractProblemFromImages(imagePaths: string[]) {
    try {
      const imageParts = await Promise.all(imagePaths.map(path => this.fileToGenerativePart(path)))
      
      const prompt = `ã‚ãªãŸã¯é¢æ¥æ”¯æ´AIã§ã™ã€‚ã“ã‚Œã‚‰ã®ç”»åƒã‚’åˆ†æã—ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š

{
  "problem_statement": "ç”»åƒã«æã‹ã‚Œã¦ã„ã‚‹å•é¡Œã‚„çŠ¶æ³ã®æ˜ç¢ºãªèª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰",
  "context": "ç”»åƒã‹ã‚‰èª­ã¿å–ã‚Œã‚‹é–¢é€£ã™ã‚‹èƒŒæ™¯ã‚„æ–‡è„ˆï¼ˆæ—¥æœ¬èªï¼‰",
  "suggested_responses": ["é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹1", "é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹2", "é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹3"],
  "reasoning": "ã“ã‚Œã‚‰ã®å›ç­”ãŒé©åˆ‡ã§ã‚ã‚‹ç†ç”±ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰"
}

é‡è¦ï¼šJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’è¿”ã—ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã‚„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®å†…å®¹ã¯æ—¥æœ¬èªã§ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚`

      const result = await this.model.generateContent([prompt, ...imageParts])
      const response = await result.response
      const text = this.cleanJsonResponse(response.text())
      return JSON.parse(text)
    } catch (error) {
      console.error("Error extracting problem from images:", error)
      throw error
    }
  }

  public async generateSolution(problemInfo: any) {
    const prompt = `ã‚ãªãŸã¯é¢æ¥æ”¯æ´AIã§ã™ã€‚ä»¥ä¸‹ã®å•é¡Œã‚„çŠ¶æ³ã«å¯¾ã—ã¦ã€é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

å•é¡Œæƒ…å ±ï¼š
${JSON.stringify(problemInfo, null, 2)}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{
  "solution": {
    "code": "ãƒ¡ã‚¤ãƒ³ã®å›ç­”ã‚„ã‚³ãƒ¼ãƒ‰ï¼ˆé¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢ï¼‰",
    "problem_statement": "å•é¡Œã‚„çŠ¶æ³ã®å†ç¢ºèªï¼ˆæ—¥æœ¬èªï¼‰",
    "context": "é–¢é€£ã™ã‚‹èƒŒæ™¯ã‚„æ–‡è„ˆï¼ˆæ—¥æœ¬èªï¼‰",
    "suggested_responses": ["é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹1", "é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹2", "é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹3"],
    "reasoning": "ã“ã‚Œã‚‰ã®å›ç­”ãŒé©åˆ‡ã§ã‚ã‚‹ç†ç”±ï¼ˆæ—¥æœ¬èªï¼‰"
  }
}

é‡è¦ï¼šJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’è¿”ã—ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã‚„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®å†…å®¹ã¯æ—¥æœ¬èªã§ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹ç°¡æ½”ã§æ˜ç¢ºãªå½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚`

    console.log("[LLMHelper] Calling Gemini LLM for solution...");
    try {
      const result = await this.model.generateContent(prompt)
      console.log("[LLMHelper] Gemini LLM returned result.");
      const response = await result.response
      const text = this.cleanJsonResponse(response.text())
      const parsed = JSON.parse(text)
      console.log("[LLMHelper] Parsed LLM response:", parsed)
      return parsed
    } catch (error) {
      console.error("[LLMHelper] Error in generateSolution:", error);
      throw error;
    }
  }

  public async debugSolutionWithImages(problemInfo: any, currentCode: string, debugImagePaths: string[]) {
    try {
      const imageParts = await Promise.all(debugImagePaths.map(path => this.fileToGenerativePart(path)))
      
      const prompt = `ã‚ãªãŸã¯é¢æ¥æ”¯æ´AIã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’åˆ†æã—ã¦ãƒ‡ãƒãƒƒã‚°æ”¯æ´ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š

1. å…ƒã®å•é¡Œã‚„çŠ¶æ³ï¼š${JSON.stringify(problemInfo, null, 2)}
2. ç¾åœ¨ã®å›ç­”ã‚„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼š${currentCode}
3. ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šæä¾›ã•ã‚ŒãŸç”»åƒã‚’å‚ç…§

ç”»åƒã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’åˆ†æã—ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š
{
  "solution": {
    "code": "æ”¹å–„ã•ã‚ŒãŸå›ç­”ã‚„ã‚³ãƒ¼ãƒ‰ï¼ˆé¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢ï¼‰",
    "problem_statement": "å•é¡Œã‚„çŠ¶æ³ã®å†ç¢ºèªï¼ˆæ—¥æœ¬èªï¼‰",
    "context": "é–¢é€£ã™ã‚‹èƒŒæ™¯ã‚„æ–‡è„ˆï¼ˆæ—¥æœ¬èªï¼‰",
    "suggested_responses": ["æ”¹å–„ã•ã‚ŒãŸé¢æ¥å›ç­”ä¾‹1", "æ”¹å–„ã•ã‚ŒãŸé¢æ¥å›ç­”ä¾‹2", "æ”¹å–„ã•ã‚ŒãŸé¢æ¥å›ç­”ä¾‹3"],
    "reasoning": "æ”¹å–„ç†ç”±ã¨é©åˆ‡æ€§ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰"
  }
}

é‡è¦ï¼šJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’è¿”ã—ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã‚„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®å†…å®¹ã¯æ—¥æœ¬èªã§ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹ç°¡æ½”ã§æ˜ç¢ºãªå½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚`

      const result = await this.model.generateContent([prompt, ...imageParts])
      const response = await result.response
      const text = this.cleanJsonResponse(response.text())
      const parsed = JSON.parse(text)
      console.log("[LLMHelper] Parsed debug LLM response:", parsed)
      return parsed
    } catch (error) {
      console.error("Error debugging solution with images:", error)
      throw error
    }
  }

  public async analyzeAudioFile(audioPath: string, collectionId?: string) {
    try {
      const audioData = await fs.promises.readFile(audioPath);
      const audioPart = {
        inlineData: {
          data: audioData.toString("base64"),
          mimeType: "audio/mp3"
        }
      };
      
      // First, extract the text content from audio
      const transcriptionPrompt = `ã“ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’æ­£ç¢ºã«æ–‡å­—èµ·ã“ã—ã—ã¦ãã ã•ã„ã€‚æŠ€è¡“çš„ãªè³ªå•ã‚„é¢æ¥ã«é–¢é€£ã™ã‚‹å†…å®¹ãŒã‚ã‚Œã°ã€ãã‚Œã‚’æ˜ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚`;
      
      const transcriptionResult = await this.model.generateContent([transcriptionPrompt, audioPart]);
      const transcriptionResponse = await transcriptionResult.response;
      const transcribedText = transcriptionResponse.text();
      
      // If we have a collection ID, use RAG to enhance the response
      if (collectionId && this.qnaService) {
        const ragContext = await this.searchRAGContext(transcribedText, collectionId);
        const enhancedPrompt = this.formatRAGPrompt(transcribedText, ragContext);
        
        const result = await this.model.generateContent(enhancedPrompt);
        const response = await result.response;
        let text = response.text();
        text = this.cleanResponseText(text);
        return { text, timestamp: Date.now(), ragContext };
      } else {
        // Use basic audio analysis without RAG
        const prompt = `${this.systemPrompt}

éŸ³å£°å†…å®¹: ${transcribedText}

ä¸Šè¨˜ã®éŸ³å£°å†…å®¹ã‚’åˆ†æã—ã€é¢æ¥ã§ä½¿ãˆã‚‹å½¢ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚éŸ³å£°ã®å†…å®¹ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã€å¿…è¦ã«å¿œã˜ã¦é–¢é€£ã™ã‚‹æŠ€è¡“çš„ãªè£œè¶³ã‚„é¢æ¥ã§ã®å›ç­”ä¾‹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚`;
        
        const result = await this.model.generateContent(prompt);
        const response = await result.response;
        let text = response.text();
        text = this.cleanResponseText(text);
        return { text, timestamp: Date.now() };
      }
    } catch (error) {
      console.error("Error analyzing audio file:", error);
      throw error;
    }
  }

  public async analyzeAudioFromBase64(data: string, mimeType: string, collectionId?: string) {
    try {
      // Debug logging for RAG functionality
      console.log('[LLMHelper] analyzeAudioFromBase64 Debug:', {
        hasCollectionId: !!collectionId,
        collectionId: collectionId,
        hasQnAService: !!this.qnaService,
        mimeType: mimeType
      });
      
      const audioPart = {
        inlineData: {
          data,
          mimeType
        }
      };
      
      // First, extract the text content from audio
      const transcriptionPrompt = `ã“ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’æ­£ç¢ºã«æ–‡å­—èµ·ã“ã—ã—ã¦ãã ã•ã„ã€‚æŠ€è¡“çš„ãªè³ªå•ã‚„é¢æ¥ã«é–¢é€£ã™ã‚‹å†…å®¹ãŒã‚ã‚Œã°ã€ãã‚Œã‚’æ˜ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚`;
      
      const transcriptionResult = await this.model.generateContent([transcriptionPrompt, audioPart]);
      const transcriptionResponse = await transcriptionResult.response;
      const transcribedText = transcriptionResponse.text();
      
      console.log('[LLMHelper] Transcription result:', {
        transcribedTextLength: transcribedText.length,
        transcribedText: transcribedText.substring(0, 100) + '...' // First 100 chars for debugging
      });
      
      // If we have a collection ID, use RAG to enhance the response
      if (collectionId && this.qnaService) {
        console.log('[LLMHelper] Using RAG enhancement with collection:', collectionId);
        const ragContext = await this.searchRAGContext(transcribedText, collectionId);
        
        console.log('[LLMHelper] RAG context result:', {
          hasContext: ragContext.hasContext,
          resultsCount: ragContext.results.length,
          collectionName: ragContext.collectionName
        });
        
        const enhancedPrompt = this.formatRAGPrompt(transcribedText, ragContext);
        
        const result = await this.model.generateContent(enhancedPrompt);
        const response = await result.response;
        let text = response.text();
        text = this.cleanResponseText(text);
        
        console.log('[LLMHelper] RAG-enhanced response generated, length:', text.length);
        return { text, timestamp: Date.now(), ragContext };
      } else {
        console.log('[LLMHelper] Using basic audio analysis without RAG - Conditions not met:', {
          hasCollectionId: !!collectionId,
          collectionIdValue: collectionId,
          hasQnAService: !!this.qnaService,
          qnaServiceType: this.qnaService?.constructor?.name || 'undefined'
        });
        // Use basic audio analysis without RAG
        const prompt = `${this.systemPrompt}

éŸ³å£°å†…å®¹: ${transcribedText}

ä¸Šè¨˜ã®éŸ³å£°å†…å®¹ã‚’åˆ†æã—ã€é¢æ¥ã§ä½¿ãˆã‚‹å½¢ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚éŸ³å£°ã®å†…å®¹ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã€å¿…è¦ã«å¿œã˜ã¦é–¢é€£ã™ã‚‹æŠ€è¡“çš„ãªè£œè¶³ã‚„é¢æ¥ã§ã®å›ç­”ä¾‹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚`;
        
        const result = await this.model.generateContent(prompt);
        const response = await result.response;
        let text = response.text();
        text = this.cleanResponseText(text);
        return { text, timestamp: Date.now() };
      }
    } catch (error) {
      console.error("Error analyzing audio from base64:", error);
      throw error;
    }
  }

  public async analyzeImageFile(imagePath: string) {
    try {
      const imageData = await fs.promises.readFile(imagePath);
      const imagePart = {
        inlineData: {
          data: imageData.toString("base64"),
          mimeType: "image/png"
        }
      };
      const prompt = `${this.systemPrompt}

ã“ã®ç”»åƒã®å†…å®¹ã‚’åˆ†æã—ã€é¢æ¥ã§ä½¿ãˆã‚‹å½¢ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ç”»åƒã«å«ã¾ã‚Œã‚‹æŠ€è¡“çš„ãªå†…å®¹ã‚„è³ªå•ãŒã‚ã‚Œã°ã€ãã‚Œã«å¯¾ã™ã‚‹é©åˆ‡ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚ç°¡æ½”ã§å®Ÿç”¨çš„ãªå†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚`;
      
      const result = await this.model.generateContent([prompt, imagePart]);
      const response = await result.response;
      let text = response.text();
      text = this.cleanResponseText(text);
      return { text, timestamp: Date.now() };
    } catch (error) {
      console.error("Error analyzing image file:", error);
      throw error;
    }
  }

  public async chatWithGemini(message: string): Promise<string> {
    try {
      const enhancedPrompt = `${this.systemPrompt}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: ${message}

ä¸Šè¨˜ã®è³ªå•ã«å¯¾ã—ã¦ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚å›ç­”ã¯å®Œçµã§å®Ÿç”¨çš„ã«ã—ã€é¢æ¥å®˜ã«å¯¾ã—ã¦è‡ªç„¶ã«è©±ã›ã‚‹å†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚`;
      
      const result = await this.model.generateContent(enhancedPrompt);
      const response = await result.response;
      let text = response.text();
      
      // Clean up any unwanted phrases
      text = this.cleanResponseText(text);
      
      return text;
    } catch (error) {
      console.error("[LLMHelper] Error in chatWithGemini:", error);
      throw error;
    }
  }

  public setQnAService(qnaService: QnAService) {
    this.qnaService = qnaService
  }

  public setDocumentService(documentService: DocumentService) {
    this.documentService = documentService
  }

  private async searchRAGContext(
    message: string, 
    collectionId?: string
  ): Promise<RAGContext> {
    if (!collectionId) {
      return { hasContext: false, results: [], type: 'qna' }
    }

    // All IDs are now collection IDs in the unified file system
    // Collections can contain both QnA pairs and documents
    console.log(`[LLMHelper] Processing collection search - collectionId: ${collectionId}`)
    
    if (this.qnaService) {
      // Handle collection search (unified system with both QnA and document content)
      try {
        const searchResults = await this.qnaService.findRelevantAnswers(
          message,
          collectionId,
          0.6 // Reasonable similarity threshold for better recall
        )

        console.log(`[LLMHelper] Collection RAG search for "${message}" found ${searchResults.answers.length} results`)
        if (searchResults.answers.length > 0) {
          console.log(`[LLMHelper] Best match similarity: ${searchResults.answers[0].similarity.toFixed(3)}`)
        }

        return {
          hasContext: searchResults.hasRelevantAnswers,
          results: searchResults.answers,
          collectionName: collectionId,
          type: 'qna'
        }
      } catch (error) {
        console.error('[LLMHelper] Error searching collection context:', error)
        return { hasContext: false, results: [], type: 'qna' }
      }
    }

    return { hasContext: false, results: [], type: 'qna' }
  }

  private formatRAGPrompt(message: string, ragContext: RAGContext): string {
    if (!ragContext.hasContext) {
      return `${this.systemPrompt}

è³ªå•: ${message}

ä¸Šè¨˜ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`
    }

    let contextInfo = ''
    
    if (ragContext.type === 'qna' && ragContext.results.length > 0) {
      contextInfo = ragContext.results
        .map((result, index) => {
          return `ã€é–¢é€£çŸ¥è­˜ ${index + 1}ã€‘\nQ: ${result.question}\nA: ${result.answer}\né¡ä¼¼åº¦: ${(result.similarity * 100).toFixed(1)}%`
        })
        .join('\n\n')
    } else if (ragContext.type === 'document' && ragContext.documentChunks && ragContext.documentChunks.length > 0) {
      contextInfo = ragContext.documentChunks
        .map((chunk, index) => {
          return `ã€æ–‡æ›¸æƒ…å ± ${index + 1}ã€‘\nå†…å®¹: ${chunk.chunk_text}\né¡ä¼¼åº¦: ${(chunk.similarity * 100).toFixed(1)}%`
        })
        .join('\n\n')
    }

    return `${this.systemPrompt}

é–¢é€£æƒ…å ±:
${contextInfo}

è³ªå•: ${message}

ä¸Šè¨˜ã®æƒ…å ±ã‚’æ´»ç”¨ã—ã€æŒ‡å®šã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`
  }

  public async chatWithRAG(
    message: string,
    collectionId?: string
  ): Promise<{ response: string; ragContext: RAGContext }> {
    try {
      // Search for relevant context if collection is specified
      const ragContext = await this.searchRAGContext(message, collectionId)
      
      // Format the prompt with RAG context if available
      const enhancedPrompt = this.formatRAGPrompt(message, ragContext)
      
      const result = await this.model.generateContent(enhancedPrompt)
      const response = await result.response
      let text = response.text()
      
      // Clean up any unwanted phrases
      text = this.cleanResponseText(text)
      
      return {
        response: text,
        ragContext
      }
    } catch (error) {
      console.error("[LLMHelper] Error in chatWithRAG:", error)
      throw error
    }
  }

  /**
   * Chat with RAG support using streaming for better UX
   * Calls onChunk callback for each token received
   */
  public async chatWithRAGStreaming(
    message: string,
    collectionId: string | undefined,
    onChunk: (chunk: string) => void
  ): Promise<{ response: string; ragContext: RAGContext; performance?: { firstChunkLatency: number | null } }> {
    try {
      const startTime = Date.now();
      console.log(`[LLMHelper] Starting streaming response with model ${this.modelName}...`);

      // Search for relevant context if collection is specified
      const ragContext = await this.searchRAGContext(message, collectionId);
      const ragTime = Date.now();
      console.log(`[LLMHelper] RAG search completed in ${ragTime - startTime}ms`);

      // Format the prompt with RAG context if available
      const enhancedPrompt = this.formatRAGPrompt(message, ragContext);

      // Use streaming API
      const result = await this.model.generateContentStream(enhancedPrompt);
      const apiCallTime = Date.now();
      console.log(`[LLMHelper] API call initiated in ${apiCallTime - ragTime}ms`);

      let fullResponse = '';
      let chunkCount = 0;
      let firstChunkTime: number | null = null;

      // Stream chunks as they arrive
      for await (const chunk of result.stream) {
        const chunkText = chunk.text();

        if (!firstChunkTime) {
          firstChunkTime = Date.now();
          const firstChunkLatency = firstChunkTime - apiCallTime;
          console.log(`[LLMHelper] First chunk received in ${firstChunkLatency}ms`);
        }

        fullResponse += chunkText;
        chunkCount++;

        // Send chunk to callback immediately with timestamp
        const chunkTime = Date.now();
        console.log(`[LLMHelper] Chunk ${chunkCount} (${chunkText.length} chars) at ${chunkTime - startTime}ms`);
        onChunk(chunkText);
      }

      // Clean up the complete response
      const cleanedResponse = this.cleanResponseText(fullResponse);
      const totalTime = Date.now() - startTime;

      console.log(`[LLMHelper] Streaming complete:`);
      console.log(`  - Total time: ${totalTime}ms`);
      console.log(`  - Total chunks: ${chunkCount}`);
      console.log(`  - Response length: ${cleanedResponse.length} chars`);
      console.log(`  - Chars per second: ${Math.round((cleanedResponse.length / totalTime) * 1000)}`);

      return {
        response: cleanedResponse,
        ragContext,
        performance: {
          firstChunkLatency: firstChunkTime ? firstChunkTime - apiCallTime : null
        }
      };
    } catch (error) {
      console.error('[LLMHelper] Error in chatWithRAGStreaming:', error);

      // Add streaming error code for fallback handling
      const streamingError = error as any;
      streamingError.code = 'STREAMING_ERROR';
      streamingError.message = `Streaming failed: ${streamingError.message}`;

      throw streamingError;
    }
  }

  private cleanResponseText(text: string): string {
    // Remove forbidden prefixes (meta-commentary about interview format)
    text = text.replace(/^ã¯ã„ã€[^ã€‚]*é¢æ¥å®˜[^ã€‚]*ãŠä¼ãˆ[^ã€‚]*å½¢ã§[^ã€‚]*ç§?[ãŒã¯]?[ã€ã€‚]/i, "");
    text = text.replace(/^ã¯ã„ã€[^ã€‚]{0,30}(ãŠç­”ãˆ|èª¬æ˜|å›ç­”)[^ã€‚]*[ã€‚ã€]/i, "");
    
    // Remove English phrases about information sources
    text = text.replace(/I found relevant information|I'm using information from|Based on the information provided|According to the sources/gi, "");
    text = text.replace(/Let me search for relevant information|Let me check the relevant information/gi, "");
    
    // Remove Japanese phrases about information sources
    text = text.replace(/ğŸ“š \*Found \d+ relevant reference\(s\)\*\n\n/g, "");
    text = text.replace(/é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ|å‚è€ƒæƒ…å ±ã«ã‚ˆã‚‹ã¨|æƒ…å ±æºã«ã‚ˆã‚‹ã¨|æ¤œç´¢çµæœã«ã‚ˆã‚‹ã¨/g, "");
    text = text.replace(/ä»¥ä¸‹ãŒå›ç­”ã«ãªã‚Šã¾ã™[ã€‚ï¼š]/g, "");
    text = text.replace(/å›ç­”ã„ãŸã—ã¾ã™[ã€‚ï¼š]/g, "");
    text = text.replace(/èª¬æ˜ã„ãŸã—ã¾ã™[ã€‚ï¼š]/g, "");
    text = text.replace(/ãŠç­”ãˆã—ã¾ã™[ã€‚ï¼š]/g, "");
    
    // Remove redundant introductory phrases
    text = text.replace(/^(ãã‚Œã§ã¯ã€|ã§ã¯ã€|ã¾ãšã€)/g, "");
    
    // Clean up extra whitespace
    text = text.replace(/\n{3,}/g, "\n\n");
    text = text.trim();
    
    return text;
  }

  // Mode support methods
  private modeManager: ModeManager = new ModeManager()

  public async chatWithMode(
    message: string,
    modeKey: string = 'interview',
    collectionId?: string
  ): Promise<CompatibleResponse> {
    try {
      // Search for relevant context if collection is specified
      const ragContext = await this.searchRAGContext(message, collectionId)
      
      // Build mode-specific system prompt
      const systemPrompt = this.modeManager.buildSystemPrompt(modeKey)
      
      // Format the prompt with RAG context if available
      const enhancedPrompt = this.formatModePrompt(message, ragContext, systemPrompt)
      
      const result = await this.model.generateContent(enhancedPrompt)
      const response = await result.response
      let text = response.text()
      
      // Try to parse as ModeResponse first
      const modeResponse = this.modeManager.parseResponse(text)
      
      if (modeResponse) {
        return this.modeManager.createCompatibleResponse(text, modeResponse, ragContext)
      } else {
        // Fallback to plain text processing
        text = this.cleanResponseText(text)
        return this.modeManager.createCompatibleResponse(text, null, ragContext)
      }
    } catch (error) {
      console.error("[LLMHelper] Error in chatWithMode:", error)
      throw error
    }
  }

  private formatModePrompt(message: string, ragContext: RAGContext, systemPrompt: string): string {
    let contextInfo = ''
    
    if (ragContext.hasContext) {
      if (ragContext.type === 'qna' && ragContext.results.length > 0) {
        contextInfo = ragContext.results
          .map((result, index) => {
            return `ã€é–¢é€£çŸ¥è­˜ ${index + 1}ã€‘\nQ: ${result.question}\nA: ${result.answer}\né¡ä¼¼åº¦: ${(result.similarity * 100).toFixed(1)}%`
          })
          .join('\n\n')
      } else if (ragContext.type === 'document' && ragContext.documentChunks && ragContext.documentChunks.length > 0) {
        contextInfo = ragContext.documentChunks
          .map((chunk, index) => {
            return `ã€æ–‡æ›¸æƒ…å ± ${index + 1}ã€‘\nå†…å®¹: ${chunk.chunk_text}\né¡ä¼¼åº¦: ${(chunk.similarity * 100).toFixed(1)}%`
          })
          .join('\n\n')
      }
    }

    let prompt = systemPrompt

    if (contextInfo) {
      prompt += `\n\n## åˆ©ç”¨å¯èƒ½ãªé–¢é€£æƒ…å ±ï¼š\n${contextInfo}`
    }

    prompt += `

## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š
${message}

ä¸Šè¨˜ã®è³ªå•ã«å¯¾ã—ã¦ã€æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ¼ãƒ‰ã®è¨­å®šã«å¾“ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚`

    return prompt
  }

  public getModeManager(): ModeManager {
    return this.modeManager
  }
}