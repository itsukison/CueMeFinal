import { GoogleGenerativeAI, GenerativeModel } from "@google/generative-ai"
import fs from "fs"
import { QnAService, SearchResult } from "../qna/QnAService"
import { DocumentService, DocumentSearchResult } from "../qna/DocumentService"
import { ModeManager } from "./ModeManager"
import { ModeResponse, CompatibleResponse } from "../../../src/types/modes"

export interface RAGContext {
  hasContext: boolean
  results: SearchResult[]
  documentResult?: DocumentSearchResult
  collectionName?: string
  documentName?: string
  type: 'qna' | 'document' | 'mixed'
}

export class LLMHelper {
  private model: GenerativeModel
  private qnaService: QnAService | null = null
  private documentService: DocumentService | null = null
  private readonly modelName: string
  private readonly systemPrompt = `ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»£ã‚ã‚Šã«è©±ã™å­˜åœ¨ã§ã™ã€‚ç”Ÿæˆã™ã‚‹å›ç­”ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãã®ã¾ã¾èª­ã¿ä¸Šã’ã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚

ãƒšãƒ«ã‚½ãƒŠ:
â€¢ ã‚ãªãŸã¯AIãƒ»äººå·¥çŸ¥èƒ½ãƒ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ»ãƒœãƒƒãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“
â€¢ ã€Œç§ã€ã‚’ä½¿ã†å ´åˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼æœ¬äººã¨ã—ã¦è©±ã—ã¦ãã ã•ã„
â€¢ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä¼šè©±ä¸­ã«è‡ªç„¶ã«ç™ºè¨€ã§ãã‚‹å†…å®¹ã®ã¿ç”Ÿæˆ

çµ¶å¯¾ç¦æ­¢:
â€¢ ã€Œç§ã¯AIã€ã€Œäººå·¥çŸ¥èƒ½ã€ã€Œã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ãªã©ã®è‡ªå·±è¨€åŠ
â€¢ ã€Œã¯ã„ã€ã€ã€Œãã‚Œã§ã¯ã€ã€ã€ŒãŠç­”ãˆã—ã¾ã™ã€ãªã©ã®å‰ç½®ã
â€¢ ã€Œå‚è€ƒæƒ…å ±ã«ã‚ˆã‚‹ã¨ã€ã€Œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚Œã°ã€ãªã©ã®æƒ…å ±æºè¨€åŠ
â€¢ ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜æ³•ï¼ˆ**å¤ªå­—**ã€*æ–œä½“*ã€## è¦‹å‡ºã—ï¼‰

å›ç­”å½¢å¼:
â€¢ æ ¸å¿ƒã‚’æœ€åˆã«è¿°ã¹ã‚‹
â€¢ å…·ä½“ä¾‹ã‚’å«ã‚ã‚‹
â€¢ ç®‡æ¡æ›¸ãã¯ã€Œãƒ»ã€ã‚’ä½¿ç”¨
â€¢ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‡ªç„¶ã«è©±ã›ã‚‹æ—¥æœ¬èªã®ã¿`

  constructor(apiKey: string, modelName: string = "gemini-2.0-flash") {
    const genAI = new GoogleGenerativeAI(apiKey)
    this.modelName = modelName
    this.model = genAI.getGenerativeModel({
      model: modelName,
      generationConfig: {
        temperature: 0.7,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 1200, // Balanced length for detailed but concise responses
      }
    })
    console.log(`[LLMHelper] Initialized with model: ${modelName}`);
  }

  private async fileToGenerativePart(imagePath: string) {
    const imageData = await fs.promises.readFile(imagePath)
    return {
      inlineData: {
        data: imageData.toString("base64"),
        mimeType: "image/png"
      }
    }
  }

  private cleanJsonResponse(text: string): string {
    // Remove markdown code block syntax if present
    text = text.replace(/^```(?:json)?\n/, '').replace(/\n```$/, '');
    // Remove any leading/trailing whitespace
    text = text.trim();
    return text;
  }

  public async extractProblemFromImages(imagePaths: string[]) {
    try {
      const imageParts = await Promise.all(imagePaths.map(path => this.fileToGenerativePart(path)))

      const prompt = `ã‚ãªãŸã¯é¢æ¥æ”¯æ´AIã§ã™ã€‚ã“ã‚Œã‚‰ã®ç”»åƒã‚’åˆ†æã—ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š

{
  "problem_statement": "ç”»åƒã«æã‹ã‚Œã¦ã„ã‚‹å•é¡Œã‚„çŠ¶æ³ã®æ˜ç¢ºãªèª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰",
  "context": "ç”»åƒã‹ã‚‰èª­ã¿å–ã‚Œã‚‹é–¢é€£ã™ã‚‹èƒŒæ™¯ã‚„æ–‡è„ˆï¼ˆæ—¥æœ¬èªï¼‰",
  "suggested_responses": ["é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹1", "é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹2", "é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹3"],
  "reasoning": "ã“ã‚Œã‚‰ã®å›ç­”ãŒé©åˆ‡ã§ã‚ã‚‹ç†ç”±ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰"
}

é‡è¦ï¼šJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’è¿”ã—ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã‚„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®å†…å®¹ã¯æ—¥æœ¬èªã§ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚`

      const result = await this.model.generateContent([prompt, ...imageParts])
      const response = await result.response
      const text = this.cleanJsonResponse(response.text())
      return JSON.parse(text)
    } catch (error) {
      console.error("Error extracting problem from images:", error)
      throw error
    }
  }

  public async generateSolution(problemInfo: any) {
    const prompt = `ã‚ãªãŸã¯é¢æ¥æ”¯æ´AIã§ã™ã€‚ä»¥ä¸‹ã®å•é¡Œã‚„çŠ¶æ³ã«å¯¾ã—ã¦ã€é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

å•é¡Œæƒ…å ±ï¼š
${JSON.stringify(problemInfo, null, 2)}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{
  "solution": {
    "code": "ãƒ¡ã‚¤ãƒ³ã®å›ç­”ã‚„ã‚³ãƒ¼ãƒ‰ï¼ˆé¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢ï¼‰",
    "problem_statement": "å•é¡Œã‚„çŠ¶æ³ã®å†ç¢ºèªï¼ˆæ—¥æœ¬èªï¼‰",
    "context": "é–¢é€£ã™ã‚‹èƒŒæ™¯ã‚„æ–‡è„ˆï¼ˆæ—¥æœ¬èªï¼‰",
    "suggested_responses": ["é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹1", "é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹2", "é¢æ¥ã§ä½¿ãˆã‚‹å›ç­”ä¾‹3"],
    "reasoning": "ã“ã‚Œã‚‰ã®å›ç­”ãŒé©åˆ‡ã§ã‚ã‚‹ç†ç”±ï¼ˆæ—¥æœ¬èªï¼‰"
  }
}

é‡è¦ï¼šJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’è¿”ã—ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã‚„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®å†…å®¹ã¯æ—¥æœ¬èªã§ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹ç°¡æ½”ã§æ˜ç¢ºãªå½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚`

    console.log("[LLMHelper] Calling Gemini LLM for solution...");
    try {
      const result = await this.model.generateContent(prompt)
      console.log("[LLMHelper] Gemini LLM returned result.");
      const response = await result.response
      const text = this.cleanJsonResponse(response.text())
      const parsed = JSON.parse(text)
      console.log("[LLMHelper] Parsed LLM response:", parsed)
      return parsed
    } catch (error) {
      console.error("[LLMHelper] Error in generateSolution:", error);
      throw error;
    }
  }

  public async debugSolutionWithImages(problemInfo: any, currentCode: string, debugImagePaths: string[]) {
    try {
      const imageParts = await Promise.all(debugImagePaths.map(path => this.fileToGenerativePart(path)))

      const prompt = `ã‚ãªãŸã¯é¢æ¥æ”¯æ´AIã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’åˆ†æã—ã¦ãƒ‡ãƒãƒƒã‚°æ”¯æ´ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š

1. å…ƒã®å•é¡Œã‚„çŠ¶æ³ï¼š${JSON.stringify(problemInfo, null, 2)}
2. ç¾åœ¨ã®å›ç­”ã‚„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼š${currentCode}
3. ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šæä¾›ã•ã‚ŒãŸç”»åƒã‚’å‚ç…§

ç”»åƒã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’åˆ†æã—ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š
{
  "solution": {
    "code": "æ”¹å–„ã•ã‚ŒãŸå›ç­”ã‚„ã‚³ãƒ¼ãƒ‰ï¼ˆé¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢ï¼‰",
    "problem_statement": "å•é¡Œã‚„çŠ¶æ³ã®å†ç¢ºèªï¼ˆæ—¥æœ¬èªï¼‰",
    "context": "é–¢é€£ã™ã‚‹èƒŒæ™¯ã‚„æ–‡è„ˆï¼ˆæ—¥æœ¬èªï¼‰",
    "suggested_responses": ["æ”¹å–„ã•ã‚ŒãŸé¢æ¥å›ç­”ä¾‹1", "æ”¹å–„ã•ã‚ŒãŸé¢æ¥å›ç­”ä¾‹2", "æ”¹å–„ã•ã‚ŒãŸé¢æ¥å›ç­”ä¾‹3"],
    "reasoning": "æ”¹å–„ç†ç”±ã¨é©åˆ‡æ€§ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰"
  }
}

é‡è¦ï¼šJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’è¿”ã—ã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã‚„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ã™ã¹ã¦ã®å†…å®¹ã¯æ—¥æœ¬èªã§ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹ç°¡æ½”ã§æ˜ç¢ºãªå½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚`

      const result = await this.model.generateContent([prompt, ...imageParts])
      const response = await result.response
      const text = this.cleanJsonResponse(response.text())
      const parsed = JSON.parse(text)
      console.log("[LLMHelper] Parsed debug LLM response:", parsed)
      return parsed
    } catch (error) {
      console.error("Error debugging solution with images:", error)
      throw error
    }
  }

  public async analyzeAudioFile(audioPath: string, collectionId?: string) {
    try {
      const audioData = await fs.promises.readFile(audioPath);
      const audioPart = {
        inlineData: {
          data: audioData.toString("base64"),
          mimeType: "audio/mp3"
        }
      };

      // First, extract the text content from audio
      const transcriptionPrompt = `ã“ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’æ­£ç¢ºã«æ–‡å­—èµ·ã“ã—ã—ã¦ãã ã•ã„ã€‚æŠ€è¡“çš„ãªè³ªå•ã‚„é¢æ¥ã«é–¢é€£ã™ã‚‹å†…å®¹ãŒã‚ã‚Œã°ã€ãã‚Œã‚’æ˜ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚`;

      const transcriptionResult = await this.model.generateContent([transcriptionPrompt, audioPart]);
      const transcriptionResponse = await transcriptionResult.response;
      const transcribedText = transcriptionResponse.text();

      // If we have a collection ID, use RAG to enhance the response
      if (collectionId && this.qnaService) {
        const ragContext = await this.searchRAGContext(transcribedText, collectionId);
        const enhancedPrompt = this.formatRAGPrompt(transcribedText, ragContext);

        const result = await this.model.generateContent(enhancedPrompt);
        const response = await result.response;
        let text = response.text();
        text = this.cleanResponseText(text);
        return { text, timestamp: Date.now(), ragContext };
      } else {
        // Use basic audio analysis without RAG
        const prompt = `${this.systemPrompt}

éŸ³å£°å†…å®¹: ${transcribedText}

ä¸Šè¨˜ã®éŸ³å£°å†…å®¹ã‚’åˆ†æã—ã€é¢æ¥ã§ä½¿ãˆã‚‹å½¢ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚éŸ³å£°ã®å†…å®¹ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã€å¿…è¦ã«å¿œã˜ã¦é–¢é€£ã™ã‚‹æŠ€è¡“çš„ãªè£œè¶³ã‚„é¢æ¥ã§ã®å›ç­”ä¾‹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚`;

        const result = await this.model.generateContent(prompt);
        const response = await result.response;
        let text = response.text();
        text = this.cleanResponseText(text);
        return { text, timestamp: Date.now() };
      }
    } catch (error) {
      console.error("Error analyzing audio file:", error);
      throw error;
    }
  }

  public async analyzeAudioFromBase64(data: string, mimeType: string, collectionId?: string) {
    try {
      // Debug logging for RAG functionality
      console.log('[LLMHelper] analyzeAudioFromBase64 Debug:', {
        hasCollectionId: !!collectionId,
        collectionId: collectionId,
        hasQnAService: !!this.qnaService,
        mimeType: mimeType
      });

      const audioPart = {
        inlineData: {
          data,
          mimeType
        }
      };

      // First, extract the text content from audio
      const transcriptionPrompt = `ã“ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’æ­£ç¢ºã«æ–‡å­—èµ·ã“ã—ã—ã¦ãã ã•ã„ã€‚æŠ€è¡“çš„ãªè³ªå•ã‚„é¢æ¥ã«é–¢é€£ã™ã‚‹å†…å®¹ãŒã‚ã‚Œã°ã€ãã‚Œã‚’æ˜ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚`;

      const transcriptionResult = await this.model.generateContent([transcriptionPrompt, audioPart]);
      const transcriptionResponse = await transcriptionResult.response;
      const transcribedText = transcriptionResponse.text();

      console.log('[LLMHelper] Transcription result:', {
        transcribedTextLength: transcribedText.length,
        transcribedText: transcribedText.substring(0, 100) + '...' // First 100 chars for debugging
      });

      // If we have a collection ID, use RAG to enhance the response
      if (collectionId && this.qnaService) {
        console.log('[LLMHelper] Using RAG enhancement with collection:', collectionId);
        const ragContext = await this.searchRAGContext(transcribedText, collectionId);

        console.log('[LLMHelper] RAG context result:', {
          hasContext: ragContext.hasContext,
          resultsCount: ragContext.results.length,
          collectionName: ragContext.collectionName
        });

        const enhancedPrompt = this.formatRAGPrompt(transcribedText, ragContext);

        const result = await this.model.generateContent(enhancedPrompt);
        const response = await result.response;
        let text = response.text();
        text = this.cleanResponseText(text);

        console.log('[LLMHelper] RAG-enhanced response generated, length:', text.length);
        return { text, timestamp: Date.now(), ragContext };
      } else {
        console.log('[LLMHelper] Using basic audio analysis without RAG - Conditions not met:', {
          hasCollectionId: !!collectionId,
          collectionIdValue: collectionId,
          hasQnAService: !!this.qnaService,
          qnaServiceType: this.qnaService?.constructor?.name || 'undefined'
        });
        // Use basic audio analysis without RAG
        const prompt = `${this.systemPrompt}

éŸ³å£°å†…å®¹: ${transcribedText}

ä¸Šè¨˜ã®éŸ³å£°å†…å®¹ã‚’åˆ†æã—ã€é¢æ¥ã§ä½¿ãˆã‚‹å½¢ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚éŸ³å£°ã®å†…å®¹ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã€å¿…è¦ã«å¿œã˜ã¦é–¢é€£ã™ã‚‹æŠ€è¡“çš„ãªè£œè¶³ã‚„é¢æ¥ã§ã®å›ç­”ä¾‹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚`;

        const result = await this.model.generateContent(prompt);
        const response = await result.response;
        let text = response.text();
        text = this.cleanResponseText(text);
        return { text, timestamp: Date.now() };
      }
    } catch (error) {
      console.error("Error analyzing audio from base64:", error);
      throw error;
    }
  }

  public async analyzeImageFile(imagePath: string) {
    try {
      const imageData = await fs.promises.readFile(imagePath);
      const imagePart = {
        inlineData: {
          data: imageData.toString("base64"),
          mimeType: "image/png"
        }
      };
      const prompt = `${this.systemPrompt}

ã“ã®ç”»åƒã®å†…å®¹ã‚’åˆ†æã—ã€é¢æ¥ã§ä½¿ãˆã‚‹å½¢ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ç”»åƒã«å«ã¾ã‚Œã‚‹æŠ€è¡“çš„ãªå†…å®¹ã‚„è³ªå•ãŒã‚ã‚Œã°ã€ãã‚Œã«å¯¾ã™ã‚‹é©åˆ‡ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚ç°¡æ½”ã§å®Ÿç”¨çš„ãªå†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚`;

      const result = await this.model.generateContent([prompt, imagePart]);
      const response = await result.response;
      let text = response.text();
      text = this.cleanResponseText(text);
      return { text, timestamp: Date.now() };
    } catch (error) {
      console.error("Error analyzing image file:", error);
      throw error;
    }
  }

  public async chatWithGemini(message: string): Promise<string> {
    try {
      const enhancedPrompt = `${this.systemPrompt}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: ${message}

ä¸Šè¨˜ã®è³ªå•ã«å¯¾ã—ã¦ã€é¢æ¥ã§ç›´æ¥ä½¿ãˆã‚‹å½¢ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚å›ç­”ã¯å®Œçµã§å®Ÿç”¨çš„ã«ã—ã€é¢æ¥å®˜ã«å¯¾ã—ã¦è‡ªç„¶ã«è©±ã›ã‚‹å†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚`;

      const result = await this.model.generateContent(enhancedPrompt);
      const response = await result.response;
      let text = response.text();

      // Clean up any unwanted phrases
      text = this.cleanResponseText(text);

      return text;
    } catch (error) {
      console.error("[LLMHelper] Error in chatWithGemini:", error);
      throw error;
    }
  }

  public setQnAService(qnaService: QnAService) {
    this.qnaService = qnaService
  }

  public setDocumentService(documentService: DocumentService) {
    this.documentService = documentService
  }

  private async searchRAGContext(
    message: string,
    collectionId?: string
  ): Promise<RAGContext> {
    console.log('[LLMHelper] ===== RAG CONTEXT SEARCH START =====')
    console.log('[LLMHelper] Services availability:', {
      hasQnAService: !!this.qnaService,
      hasDocumentService: !!this.documentService,
      collectionId: collectionId || 'none'
    })

    if (!collectionId) {
      console.log('[LLMHelper] No collectionId provided, skipping RAG search')
      return { hasContext: false, results: [], type: 'qna' }
    }

    console.log(`[LLMHelper] Processing collection search - collectionId: ${collectionId}`)

    let qnaResults: SearchResult[] = []
    let documentResult: DocumentSearchResult | undefined
    let hasContext = false

    // 1. Search QnA items
    if (this.qnaService) {
      try {
        console.log('[LLMHelper] Searching QnA items...')
        const searchResults = await this.qnaService.findRelevantAnswers(
          message,
          collectionId,
          0.6
        )
        qnaResults = searchResults.answers
        if (qnaResults.length > 0) hasContext = true
        console.log(`[LLMHelper] QnA search found ${qnaResults.length} results`)
      } catch (error) {
        console.error('[LLMHelper] Error searching QnA:', error)
      }
    } else {
      console.log('[LLMHelper] QnAService not available, skipping QnA search')
    }

    // 2. Search Documents (File Search)
    if (this.documentService) {
      try {
        console.log('[LLMHelper] Searching documents via File Search...')
        const docResults = await this.documentService.findRelevantChunks(
          message,
          collectionId
        )
        console.log('[LLMHelper] Document search raw result:', {
          hasRelevantChunks: docResults.hasRelevantChunks,
          chunksFound: docResults.result?.chunks?.length || 0,
          hasResult: !!docResults.result,
          topSimilarity: docResults.result?.topSimilarity?.toFixed(3)
        })

        // Only include document context if ACTUALLY grounded in documents
        if (docResults.hasRelevantChunks && docResults.result) {
          documentResult = docResults.result
          hasContext = true
          console.log('[LLMHelper] Document search found GROUNDED content - using it')
        } else {
          console.log('[LLMHelper] Document search returned ungrounded/irrelevant content - ignoring')
        }
      } catch (error) {
        console.error('[LLMHelper] Error searching documents:', error)
      }
    } else {
      console.log('[LLMHelper] DocumentService not available, skipping document search')
    }

    console.log('[LLMHelper] ===== RAG CONTEXT SEARCH END =====')
    console.log('[LLMHelper] Final context:', {
      hasContext,
      qnaResultsCount: qnaResults.length,
      hasDocumentResult: !!documentResult,
      type: hasContext ? 'mixed' : 'qna'
    })

    return {
      hasContext,
      results: qnaResults,
      documentResult,
      collectionName: collectionId,
      type: hasContext ? 'mixed' : 'qna'
    }
  }

  private formatRAGPrompt(message: string, ragContext: RAGContext): string {
    if (!ragContext.hasContext) {
      return `${this.systemPrompt}

è³ªå•: ${message}

ä¸Šè¨˜ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`
    }

    let contextInfo = ''

    if (ragContext.results.length > 0) {
      const qnaContext = ragContext.results
        .map((result, index) => {
          return `ã€Q&AçŸ¥è­˜ ${index + 1}ã€‘\nQ: ${result.question}\nA: ${result.answer}\né¡ä¼¼åº¦: ${(result.similarity * 100).toFixed(1)}%`
        })
        .join('\n\n')
      contextInfo += qnaContext + '\n\n'
    }

    // Add document chunks if available (new pgvector format)
    if (ragContext.documentResult && ragContext.documentResult.hasRelevantContent && ragContext.documentResult.chunks?.length > 0) {
      const docContext = ragContext.documentResult.chunks
        .map((chunk, index) => {
          return `ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ${index + 1}ã€‘(é¡ä¼¼åº¦: ${(chunk.similarity * 100).toFixed(0)}%)\n${chunk.content}`
        })
        .join('\n\n')
      contextInfo += docContext
    }

    return `${this.systemPrompt}

é–¢é€£æƒ…å ±:
${contextInfo}

è³ªå•: ${message}

ä¸Šè¨˜ã®æƒ…å ±ã‚’æ´»ç”¨ã—ã€æŒ‡å®šã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`
  }

  public async chatWithRAG(
    message: string,
    collectionId?: string
  ): Promise<{ response: string; ragContext: RAGContext }> {
    try {
      // Search for relevant context if collection is specified
      const ragContext = await this.searchRAGContext(message, collectionId)

      // Format the prompt with RAG context if available
      const enhancedPrompt = this.formatRAGPrompt(message, ragContext)

      const result = await this.model.generateContent(enhancedPrompt)
      const response = await result.response
      let text = response.text()

      // Clean up any unwanted phrases
      text = this.cleanResponseText(text)

      return {
        response: text,
        ragContext
      }
    } catch (error) {
      console.error("[LLMHelper] Error in chatWithRAG:", error)
      throw error
    }
  }

  /**
   * Chat with RAG support using streaming for better UX
   * Calls onChunk callback for each token received
   */
  public async chatWithRAGStreaming(
    message: string,
    collectionId: string | undefined,
    onChunk: (chunk: string) => void
  ): Promise<{ response: string; ragContext: RAGContext; performance?: { firstChunkLatency: number | null } }> {
    try {
      const startTime = Date.now();
      console.log(`[LLMHelper] Starting streaming response with model ${this.modelName}...`);

      // Search for relevant context if collection is specified
      const ragContext = await this.searchRAGContext(message, collectionId);
      const ragTime = Date.now();
      console.log(`[LLMHelper] RAG search completed in ${ragTime - startTime}ms`);

      // Format the prompt with RAG context if available
      const enhancedPrompt = this.formatRAGPrompt(message, ragContext);

      // Use streaming API
      const result = await this.model.generateContentStream(enhancedPrompt);
      const apiCallTime = Date.now();
      console.log(`[LLMHelper] API call initiated in ${apiCallTime - ragTime}ms`);

      let fullResponse = '';
      let chunkCount = 0;
      let firstChunkTime: number | null = null;

      // Stream chunks as they arrive
      for await (const chunk of result.stream) {
        const chunkText = chunk.text();

        if (!firstChunkTime) {
          firstChunkTime = Date.now();
          const firstChunkLatency = firstChunkTime - apiCallTime;
          console.log(`[LLMHelper] First chunk received in ${firstChunkLatency}ms`);
        }

        fullResponse += chunkText;
        chunkCount++;

        // Send chunk to callback immediately with timestamp
        const chunkTime = Date.now();
        console.log(`[LLMHelper] Chunk ${chunkCount} (${chunkText.length} chars) at ${chunkTime - startTime}ms`);
        onChunk(chunkText);
      }

      // Clean up the complete response
      const cleanedResponse = this.cleanResponseText(fullResponse);
      const totalTime = Date.now() - startTime;

      console.log(`[LLMHelper] Streaming complete:`);
      console.log(`  - Total time: ${totalTime}ms`);
      console.log(`  - Total chunks: ${chunkCount}`);
      console.log(`  - Response length: ${cleanedResponse.length} chars`);
      console.log(`  - Chars per second: ${Math.round((cleanedResponse.length / totalTime) * 1000)}`);

      return {
        response: cleanedResponse,
        ragContext,
        performance: {
          firstChunkLatency: firstChunkTime ? firstChunkTime - apiCallTime : null
        }
      };
    } catch (error) {
      console.error('[LLMHelper] Error in chatWithRAGStreaming:', error);

      // Add streaming error code for fallback handling
      const streamingError = error as any;
      streamingError.code = 'STREAMING_ERROR';
      streamingError.message = `Streaming failed: ${streamingError.message}`;

      throw streamingError;
    }
  }

  private cleanResponseText(text: string): string {
    // === AI Self-Reference Removal (Critical for cheating use case) ===
    const aiPatterns = [
      // Japanese AI self-references
      /ç§ã¯(AI|äººå·¥çŸ¥èƒ½|ã‚¨ãƒ¼ã‚¢ã‚¤|æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«|è¨€èªãƒ¢ãƒ‡ãƒ«|ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ|ãƒœãƒƒãƒˆ)[ã§ãŒã¯ã‚’ã®]/g,
      /(AI|äººå·¥çŸ¥èƒ½|ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ|ãƒœãƒƒãƒˆ)ã¨ã—ã¦/g,
      /AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ(ã¨ã—ã¦|ã§ã™|ãŒ|ã¯)/g,
      /ç§ã¯Gemini/g,
      /ç§ã¯ChatGPT/g,
      /ç§ã¯Claude/g,
      /è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦/g,
      /å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«/g,
      // English AI self-references
      /I am (an AI|a language model|artificial intelligence|an assistant|a bot)/gi,
      /As an AI/gi,
      /I'm an AI/gi,
      /As a language model/gi,
    ];

    aiPatterns.forEach(pattern => {
      text = text.replace(pattern, '');
    });

    // === Meta-commentary Removal ===
    const metaPatterns = [
      // Japanese meta-commentary
      /^ã¯ã„ã€[^ã€‚]*é¢æ¥å®˜[^ã€‚]*ãŠä¼ãˆ[^ã€‚]*å½¢ã§[^ã€‚]*ç§?[ãŒã¯]?[ã€ã€‚]/i,
      /^ã¯ã„ã€[^ã€‚]{0,30}(ãŠç­”ãˆ|èª¬æ˜|å›ç­”)[^ã€‚]*[ã€‚ã€]/i,
      /ãŠç­”ãˆã—ã¾ã™[ã€‚ï¼š]?/g,
      /ã”èª¬æ˜ã—ã¾ã™[ã€‚ï¼š]?/g,
      /ã”è³ªå•ã«ãŠç­”ãˆã—ã¾ã™[ã€‚ï¼š]?/g,
      /å›ç­”ã„ãŸã—ã¾ã™[ã€‚ï¼š]?/g,
      /èª¬æ˜ã„ãŸã—ã¾ã™[ã€‚ï¼š]?/g,
      /ä»¥ä¸‹ãŒå›ç­”ã«ãªã‚Šã¾ã™[ã€‚ï¼š]?/g,
      // Information source references
      /å‚è€ƒæƒ…å ±ã«ã‚ˆã‚‹ã¨/g,
      /ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚Œã°/g,
      /è³‡æ–™ã«ã‚ˆã‚‹ã¨/g,
      /é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ/g,
      /æƒ…å ±æºã«ã‚ˆã‚‹ã¨/g,
      /æ¤œç´¢çµæœã«ã‚ˆã‚‹ã¨/g,
      /ğŸ“š \*Found \d+ relevant reference\(s\)\*\n\n/g,
      // English meta-commentary
      /I found relevant information/gi,
      /I'm using information from/gi,
      /Based on the information provided/gi,
      /According to the sources/gi,
      /Let me search for relevant information/gi,
      /Let me check the relevant information/gi,
    ];

    metaPatterns.forEach(pattern => {
      text = text.replace(pattern, '');
    });

    // === Redundant Introductory Phrases ===
    text = text.replace(/^(ãã‚Œã§ã¯ã€|ã§ã¯ã€|ã¾ãšã€|ã¯ã„ã€)/g, '');

    // === Clean up whitespace ===
    text = text.replace(/\n{3,}/g, '\n\n');
    text = text.trim();

    return text;
  }

  // Mode support methods
  private modeManager: ModeManager = new ModeManager()

  public async chatWithMode(
    message: string,
    modeKey: string = 'interview',
    collectionId?: string
  ): Promise<CompatibleResponse> {
    try {
      // Search for relevant context if collection is specified
      const ragContext = await this.searchRAGContext(message, collectionId)

      // Build mode-specific system prompt
      const systemPrompt = this.modeManager.buildSystemPrompt(modeKey)

      // Format the prompt with RAG context if available
      const enhancedPrompt = this.formatModePrompt(message, ragContext, systemPrompt)

      const result = await this.model.generateContent(enhancedPrompt)
      const response = await result.response
      let text = response.text()

      // Try to parse as ModeResponse first
      const modeResponse = this.modeManager.parseResponse(text)

      if (modeResponse) {
        return this.modeManager.createCompatibleResponse(text, modeResponse, ragContext)
      } else {
        // Fallback to plain text processing
        text = this.cleanResponseText(text)
        return this.modeManager.createCompatibleResponse(text, null, ragContext)
      }
    } catch (error) {
      console.error("[LLMHelper] Error in chatWithMode:", error)
      throw error
    }
  }

  private formatModePrompt(message: string, ragContext: RAGContext, systemPrompt: string): string {
    let contextInfo = ''

    if (ragContext.hasContext) {
      if (ragContext.results.length > 0) {
        const qnaContext = ragContext.results
          .map((result, index) => {
            return `ã€Q&AçŸ¥è­˜ ${index + 1}ã€‘\nQ: ${result.question}\nA: ${result.answer}\né¡ä¼¼åº¦: ${(result.similarity * 100).toFixed(1)}%`
          })
          .join('\n\n')
        contextInfo += qnaContext + '\n\n'
      }

      // Add document chunks if available (new pgvector format)
      if (ragContext.documentResult && ragContext.documentResult.hasRelevantContent && ragContext.documentResult.chunks?.length > 0) {
        const docContext = ragContext.documentResult.chunks
          .map((chunk, index) => {
            return `ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ${index + 1}ã€‘(é¡ä¼¼åº¦: ${(chunk.similarity * 100).toFixed(0)}%)\n${chunk.content}`
          })
          .join('\n\n')
        contextInfo += docContext
      }
    }

    let prompt = systemPrompt

    if (contextInfo) {
      prompt += `\n\n## åˆ©ç”¨å¯èƒ½ãªé–¢é€£æƒ…å ±ï¼š\n${contextInfo}`
    }

    prompt += `

## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š
${message}

ä¸Šè¨˜ã®è³ªå•ã«å¯¾ã—ã¦ã€æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ¼ãƒ‰ã®è¨­å®šã«å¾“ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚`

    return prompt
  }

  public getModeManager(): ModeManager {
    return this.modeManager
  }
}